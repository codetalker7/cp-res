\documentclass[12pt,a4paper]{amsart}
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[section]{placeins}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{caption} 
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[mathscr]{eucal}
\usepackage{indentfirst}
\usepackage{graphicx}
\graphicspath{{./images/}}

%pseudocode
\usepackage{algpseudocode}
\usepackage{algorithm}
\algrenewcommand\algorithmicindent{1em}%

%preamble for including inkscape figures
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\usepackage{graphics}
\usepackage{tikz}
\usepackage{pict2e}
\usepackage{epic}
\usepackage{mathrsfs}
\numberwithin{equation}{section}
\usepackage[margin=2.8cm]{geometry}
\usepackage{epstopdf} 

%the following code is for a good table of contents for amsart. Finding it was not easy. for more information, check out the following two links
%https://tex.stackexchange.com/questions/51760/table-of-contents-with-indents-and-dots
%https://tex.stackexchange.com/questions/58324/adding-indents-to-the-toc-without-adding-dotted-lines-between-sections
\makeatletter
\def\@tocline#1#2#3#4#5#6#7{\relax
  \ifnum #1>\c@tocdepth % then omit
  \else
    \par \addpenalty\@secpenalty\addvspace{#2}%
    \begingroup \hyphenpenalty\@M
    \@ifempty{#4}{%
      \@tempdima\csname r@tocindent\number#1\endcsname\relax
    }{%
      \@tempdima#4\relax
    }%
    \parindent\z@ \leftskip#3\relax \advance\leftskip\@tempdima\relax
    \rightskip\@pnumwidth plus4em \parfillskip-\@pnumwidth
    #5\leavevmode\hskip-\@tempdima
      \ifcase #1
       \or\or \hskip 1em \or \hskip 2em \else \hskip 3em \fi%
      #6\nobreak\relax
    \dotfill\hbox to\@pnumwidth{\@tocpagenum{#7}}\par
    \nobreak
    \endgroup
  \fi}
\makeatother
%table of contents code finished

%packages for adding code. use \begin{lstlisting}[language=C++]\end{lstlisting} environment to write code. In text, can also use \verb|something|
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

%all about hyperlinks. include this package as the final package to avoid any problems
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
    
%to create a new numbered environment, use the syntax
%\newtheorem{name}{display}
%to start renumbering say at every section, add a [section] in the end. To have the same numbering for two environments, do as I did with Lemma below, i.e add [Th] in between.

%let the thoerems have italic text
\newtheorem{Th}{Theorem}[section]
\newtheorem{Lemma}[Th]{Lemma}
\newtheorem{Cor}{Corollary}[Th]
\newtheorem{Prop}[Th]{Proposition}

%definitions, examples, remarks and exercises will have normal text
\theoremstyle{definition}
\newtheorem{Def}{Definition}[section]
\newtheorem{Ex}{Example}[section]
\newtheorem{Remark}{Remark}[Th]
\newtheorem{Exercise}{Exercise}[section]

%using an unnumbered section for solutions
\newtheorem*{solution}{Solution}

%use the proof environment for writing proofs. I'm using a black square for QED. 
\renewcommand\qedsymbol{$\blacksquare$}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Ztwo}{\mathbb{Z}_2}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
    
%package for fonts
\usepackage{fontspec}
%main font
%\setmainfont{Chivo}

\begin{document}

\title{Mathematical Explanations}
\author{Siddhant Chaudhary}
\date{December 2019}

\maketitle
    
\begin{abstract}
In this document, we will mathematically explore all of the concepts being used. 
\end{abstract}
    
\tableofcontents

\section{Non-Linear Data Structures}
\subsection{Binary Indexed Trees} \textit{Fenwick Trees} are used to solve the problem of dynamic range sum queries efficiently. They are useful because they are much simpler to code and much faster than segment trees. Let's begin with a lemma.

\begin{Lemma}
    For any integer type \verb|i|, \verb|i&(-i)| produces the least significant bit in \verb|i|.
\end{Lemma}
\begin{proof}
    Note that negative integers are stored using the two's complement. So, if we have an $N$-bit positive integer $x$, $-x$ will have the binary representation of $2^N - x$ (equivalently, one flips the bits of $x$, and adds a $1$ to obtain the representation of $-x$). Now, suppose the least significant bit of $x$ is $l$ steps from the right. This means that all positions $0,1,...,l - 1$ steps from the right are $0$s in the binary representation of $x$. So, if we flip the bits of $x$, all these positions will be $1$. Adding a $1$, we see that the bit at position $l$ is set to $1$, and positions $0,...,l - 1$ from the right are all $0$s. Taking the and, we see that the result has only a $1$ at position $l$, proving the claim. 
\end{proof}

Moving on, we now describe the Fenwick Tree. Suppose we have an array $A[1,...,n]$ (we use one-based indexing). Let $FT[1,...,n]$ be another array; $FT[i]$, for any $i$, will store the sum of all elements in the range $[i - LS(i) + 1, i]$, where $LS(i)$ is the \textit{least-significant bit} of $i$. For example, if $i$ is a power of $2$, then $LS(i) = i$, and hence $FT[i]$ will be the sum 
$$FT[i] = \sum_{k = 1}^i A[k]$$
Here is the nice trick: given an index $b$, we want to compute 
$$\sum_{k = 1}^b A[k]$$
To do this, we compute the values $\{FT[b_0], FT[b_1],...,FT[b_k]\}$. Here, $b_0 = b$ and $b_i = b_{i - 1} - LS(b_{i - 1})$ for each suitable $i$. In simple words, starting with $b$, we strip off the least significant bit one by one. We only need to do this for $O(\log b)$ many steps. Using this, given the function $FT$ is \textit{good enough} (for example, here $FT$ is the sum function, which has many nice properties), we can compute range queries in logarithmic time. 

Let's see how this structure handles update queries. Suppose we want to update the element at index $k$. So, we want to change $A[k]$ to some new value. So, we will have to update $FT[i]$ for all those indices $i$ which include the index $k$ in their summation. In other words, we want to update all $i$ such that 
$$i - LS(i) + 1 \le k \le i$$
Then, by doing casework on $LS(i)$, we see that all such $i$ are obtained by the sequence $\{c_0,...,c_k\}$, where $c_0 = k$ and $c_{i + 1} = c_i + LS(k)$. In other words, starting from $k$, we add least significant digits \textcolor{red}{(this is one of the beautiful connections between these operations)}. So again, we only need to update $O(\log n)$ many values. 

\section{Flow Networks}
\subsection{Definitions and the Set up} In this subsection, we will formally define flow networks and prove some of their useful properties. 

\begin{Def}
	A \textit{flow network} is a graph $G = (V, E)$ with two distinguished vertices $s$ and $t$, called the \textit{source} and \textit{sink} vertices respectively. Each edge $(u, v)\in E$ has a non-negative capacity, denoted by $C(u, v)$. If $(u, v)\notin E$, we say $C(u, v) = 0$. 
\end{Def}

\begin{Def}
	A \textit{flow} on a flow network $G$ is a function $f:V\times V\to \textbf{R}$ that satisfies the following constraints. 
	\begin{enumerate}
		\item For all $u, v\in V$, $0\le f(u, v)\le c(u, v)$. This is called the \textit{capacity constraint}. 
		\item For all $u, v\in V$, $f(u, v) = -f(v , u)$. This is called the \textit{skew symmetry constraint}.  
		\item For all $u\in V - \{s, t\}$, we have 
		$$\sum_{u\in V}f(u, v) = 0$$
		This is the so called \textit{flow conservation law}. 
	\end{enumerate}
	Also, the following assumptions are made about the network $G$.
	\begin{itemize}
		\item There are no self-loops in $G$, i.e $f(u, u) = 0$ for every $u\in V$.
		\item There are no \textit{two-cycles} in $G$; formally, there is no pair $u, v$ of vertices in $G$ such that both $(u, v)$ and $(v, u)$ is an edge in $G$. Note that this is \textit{enforced} by property (2) above, since we are assuming that flows along edges are non-negative. The reason to make this assumption is for \textbf{(i)} simplification and \textbf{(ii)} networks $G$ which have two-cycles can be converted to an equivalent network $G'$ which have no two-cycles; intuitively, this is achieved by adding a new vertex and creating a three cycle. Here, \textit{equivalence} of networks means that the \textit{max-flow} (which we will define in a moment) in the two networks will be the same.
	\end{itemize}
\end{Def}

\newcommand{\netflow}[2]{f(#1, #2)}
\newcommand{\netflowExpandSingle}[2]{\sum_{u\in #2} f(#1, u)}
\newcommand{\netflowExpand}[2]{\sum_{u\in #1, v\in #2} f(u, v)}

\begin{Def}
	Let $A, B$ be any arbitrary subsets of $V$. Define the quantity $\netflow{A}{B}$ as follows. 
	$$\netflow{A}{B} := \sum_{u\in A, v\in B}f(u, v)$$
	We call this quantity the \textit{net flow} going from $A$ to $B$. The reasoning behind this naming will be proved later. 
\end{Def}

\newcommand{\flowValue}[1]{|#1|}
\begin{Def}
	The \textit{value} of a flow $f$, denoted by $\flowValue{f}$, is defined to be 
	$$\flowValue{f} = \sum_{v\in V} f(s, v) = \netflow{s}{V}$$
	Informally, this represents the net flow coming out of $s$, i.e the flow coming out of $s$ minus the flow going into $s$. This is a good definition of \textit{net flow}, as it will turn out that the net flow coming out of $s$ is equal to the net flow going into $t$. 
\end{Def}

\begin{Prop}
	Let $f$ be any flow, and let $X, Y$ and $Z$ be arbitrary subsets of $V$. Then the following hold. 
	\begin{enumerate}
		\item $\netflow{X}{X} = 0$.
		\item $\netflow{X}{Y} = -\netflow{Y}{X}$.
		\item $\netflow{X\cup Y}{Z} = \netflow{X}{Z} + \netflow{Y}{Z}$ provided $X\cap Y = \phi$. 
	\end{enumerate}
\end{Prop}
\begin{proof}
	Let us begin with (1). There is nothing more to this than simply expanding things out. 
	\begin{align*}
		f(X, X) &= \sum_{u\in X, v\in X}f(u, v)\\
		&= \sum_{u\in X}f(x, x) + \sum_{u\in X, v\in X, u \ne v}f(u , v)\\
		&= 0 + \sum_{u\ne v} f(u, v) + f(v , u)\\
		&= 0 + \sum_{u\ne v} f(u, v) - f(u , v)\\
		&= 0
	\end{align*}
	Property (2) is immediate by the definition of net flow. Property (3) is also trivial and follows from the definition.
\end{proof}

\begin{Th}
	$|f| = \netflow{V}{t}$. Informally, the flow coming out of the source is equal to the flow going into the sink.
\end{Th}
\begin{proof}
	To begin with, consider the sets $\{s\}$ and $V - \{s\}$, which are clearly disjoint. So, 
	$$\netflow{V}{V} = \netflow{s}{V} + \netflow{V - \{s\}}{V}$$
	The left hand side is clearly zero. So, we get 
	$$\netflow{s}{V} = -\netflow{V - \{s\}}{V} = \netflow{V}{V - \{s\}}$$
	Now again, consider the sets $\{t\}$ and $V - \{s,t\}$, which are clearly disjoint. So, we have the following. 
	\begin{align*}
		\netflow{V}{V - \{s\}} &= \netflow{V}{t} + \netflow{V}{V - \{s, t\}}\\
		&= \netflow{V}{t} - \netflow{V - \{s, t\}}{V}\\
		&= \netflow{V}{t} - \netflowExpand{V - \{s, t\}}{V}\\
		&= f(V, t) + 0
	\end{align*}
	where in the last step, we have used flow conservation law. 
\end{proof}

\newcommand{\cut}[2]{(#1, #2)}
\newcommand{\capacity}[2]{C(#1, #2)}
\newcommand{\capacityExpand}[2]{\sum_{u\in #1, v\in #2}c(u, v)}

\begin{Def}
	For a flow network $G$, an $\cut{S}{T}$ \textit{cut} of a flow network $G$ is a partition of $V$ such that $s\in S$ and $t\in T$. If $f$ is a flow of $G$, then the flow across the cut $\netflow{S}{T}$. The \textit{capacity} of a cut, denoted by $\capacity{S}{T}$, is defined as follows.
	$$\capacity{S}{T} := \capacityExpand{S}{T}$$
	Clearly, the flow across a cut cannot exceed the capacity of the cut, i.e 
	$$\netflow{S}{T}\le \capacity{S}{T}$$
\end{Def}

\subsection{Flow value with Cuts} We begin by proving a simple lemma. 
\begin{Lemma}
	For any flow $f$ and any cut $\cut{S}{T}$, we have 
	$$|f| = \netflow{S}{T}$$
	Informally, this is true because the source is on one side of the cut, while the sink is on the other side. 
\end{Lemma}
\begin{proof}
	The proof is straightforward. 
	\begin{align*}
		\netflow{S}{T} &= \netflow{S}{V} - \netflow{S}{S}\\
		&= \netflow{S}{V} + 0\\
		&= \netflow{s}{V} + \netflow{S - \{s\}}{V}\\
		&= |f|
	\end{align*}
	as the second value is zero by conservation of flow. 
\end{proof}

\begin{Cor}
	\label{flowUpperBound}
	If $\cut{S}{T}$ is any cut, then $|f|\le \capacity{S}{T}$. Hence, $|f|$ is less than the minimum capacity over all the cuts. 
\end{Cor}

\newcommand{\residualCap}[2]{c_f(#1, #2)}
\newcommand{\residualCapPath}[1]{c_f(#1)}
\subsection{Residual Networks} Let $G(V, E)$ be a flow network. For any pair of vertices $(u, v)$, define the \textit{residual capacity} $\residualCap{u}{v}$ as
$$\residualCap{u}{v} := c(u, v) - \netflow{u}{v}$$
The \textit{residual network} is the graph $G_f(V, E_f)$, where $E_f$ is defined to be the set of all edges with \textit{positive} residual capacity. 

\begin{Def}
	An \textit{augmenting path} is any path from $s$ to $t$ in the residual network $G_f$. The \textit{residual capacity} of an augmenting path is the minimum value of any edge along the path. Formally, 
	$$\residualCapPath{p} := \underset{(u, v)\in p}{\text{min}}\residualCap{u}{v}$$
\end{Def}

\subsection{How to augment a flow?} Suppose we are given a flow $f$, and suppose we have an augmenting path $p$ in residual network. The question is: using this augmenting path, can we somehow increase the flow? The answer is yes, and this is what we will prove in this section.

Define a flow $f_p$ on the network $G_f$ as follows. 
$$f_p(u, v) = 
	\begin{cases}
		\residualCapPath{p}\quad\quad &, \quad\text{if $(u, v)\in p$}\\
		0\quad\quad &, \quad\text{otherwise}
	\end{cases}
$$
That $f_p$ is really a flow on $G_f$ is not hard to see. Intuitively, we are just sending in the maximum possible flow that we can send along the path $p$, and nowhere else. 

Next, we define a flow $f + f_p$ on the original network $G$ as follows. 
$$(f + f_p)(u, v) = 
	\begin{cases}
		\netflow{u}{v} + f_p(u, v) - f_p(v, u)\quad\quad &,\quad\text{if $(u, v)\in E$}\\
		0\quad\quad &,\quad \text{otherwise}
	\end{cases}
$$
Try to prove that this is indeed a flow in the original network. Infact, it is also true that 
$$|f + f_p| = |f| + |f_p|$$
and hence this increments are flow. 

\subsection{Max-Flow Min-Cut Theorem}  In this section, we will prove a very important theorem, which will enable us to devise a max flow algorithm.

\begin{Th}[Max-Flow Min-Cut Theorem]
	The following are equivalent. 
	\begin{enumerate}
		\item $|f| = \capacity{S}{T}$ for some cut $\cut{S}{T}$. 
		\item $f$ is a maximum flow. 
		\item $f$ admits no augmenting paths. 
	\end{enumerate}
\end{Th}
\begin{proof}
	First, let us show that $(1)\implies(2)$. But this is immediate from \textbf{Corollary} \ref{flowUpperBound}; since $|f| = \capacity{S}{T}$ for some cut $\cut{S}{T}$, $f$ has to be a max flow, because it can't be increased. This proves the claim. 
	
	Let us now consider $(2)\implies(3)$. Suppose $f$ is a max flow. If $f$ admits an augmenting path $p$, then clearly we can increase $f$ by $\residualCapPath{p}$ and get a strictly greater flow. So, there cannot be any augmenting paths. 
	
	Finally, let us prove $(3)\implies (1)$, and this will be a little more involved, but the argument is intuitive enough. Suppose $f$ admits no augmenting paths, i.e in the residual network $G_f$, there are no paths from $s$ to $t$. With this in mind, define the set $S$ as follows. 
	$$S := \{u\in V\,\,|\,\,\text{$u$ is reachable from $s$ in $G_f$}\}$$
	Then define $T := V - S$, and clearly $\cut{S}{T}$ is an $s-t$ cut. We will show that 
	$$\netflow{S}{T} = \capacity{S}{T}$$
	and as above in $(1)\implies (2)$, this will finish our proof. The argument is intuitive; suppose $u\in S$ and $v\in T$. Clearly, the edge $(u, v)$ cannot be a part of the residual network $G_f$, by the very definition of the cut. This means that $\residualCap{u}{v} = 0$, which in turn means 
	$$\netflow{u}{v} = c(u, v)$$
	Clearly, by the definition of $\netflow{S}{T}$ and $\capacity{S}{T}$, this means that 
	$$\netflow{S}{T} = \capacity{S}{T}$$
	and hence the proof is complete. 
\end{proof}

\subsection{Edmonds-Karp Algorithm} The \textit{Edmonds-Karp} algorithm is very similar to the naive \textit{Ford-Fulkerson} algorithm, but there is a key difference in how the Edmonds-Karp algorithm finds the augmenting path. In this section, we will analyse the Edmonds-Karp algorithm, and if not specified, the flows/capacities will be assumed to be integral. 

The Edmonds-Karp algorithm can be described as follows. 
\begin{enumerate}
	\item Given $G$, compute $G_f$, the residual network.
	\item Find a shortest path from $s$ to $t$ in the residual network; this can be done using BFS. Here by \textit{shortest} we mean that each edge was weight $1$. 
	\item Having found a shortest path, augment the flow on that path. Get a new flow.
	\item Repeat the algorithm from step (1). 
\end{enumerate}
Suppose $T$ is the number of steps for which the algorithm runs. Steps (2) and (3) take $O(E)$ time, which is trivial. So, the running time is $O(TE)$. We will now show that $T = O(VE)$. 

\newcommand{\shortestResidualPath}[2]{\delta_{#1}(#2)}
\begin{Def}
	Given a residual network $G_f$, define $\shortestResidualPath{f}{v}$ to be the length of the shortest path from $s$ to $v$, for $v\in V$. 
\end{Def}

\begin{Lemma}[Monotonic]
	$\shortestResidualPath{f}{v}$ does not decrease during the algorithm execution, for all $v\in V$. So, this quantity can only increase or stay the same.
\end{Lemma}
\begin{proof}
	We prove this by contradiction. Suppose $f'$ is the new flow obtained from $f$ after a single step of the algorithm. Also, suppose there is some $v\in V$ such that 
	$$\shortestResidualPath{f'}{v} < \shortestResidualPath{f}{v}$$
	Among such violating vertices, let $\nu$ be the vertex such that $\shortestResidualPath{f'}{\nu}$ is minimum. So, there is some path $s\leadsto\nu$ in the residual graph $G_{f'}$, and suppose the path is $s\leadsto u\to\nu$, i.e $u$ is the predecessor of $\nu$ in this path. Clearly, we have 
	$$\shortestResidualPath{f'}{\nu} = \shortestResidualPath{f'}{u} + 1$$
	Also, by the definition of $\nu$, we have that 
	$$\shortestResidualPath{f'}{u} + 1\ge \shortestResidualPath{f}{u} + 1$$
	giving us the inequality 
	$$\shortestResidualPath{f'}{\nu}\ge \shortestResidualPath{f}{u} + 1$$
	Now, we will handle two cases. 
	\begin{enumerate}
		\item In the first case, suppose $(u, v)\in G_f$. In that case, by the triangle inequality of shortest paths, we have 
		$$\shortestResidualPath{f}{u} + 1\ge \shortestResidualPath{f}{\nu}$$
		which gives us the inequality 
		$$\shortestResidualPath{f'}{\nu}\ge \shortestResidualPath{f}{\nu}$$
		which is a contradiction to our assumption.
		\item In the second case, we have $(u, \nu)\notin G_f$. Also, we clearly know that $(u, \nu)\in G_{f'}$. Clearly, this is only possible if $(\nu, u)\in G_f$ (and in fact this edge lies on the augmenting path we found to get $f'$). In this case, because Edmonds-Karp only augments shortest paths, we have 
		$$\shortestResidualPath{f}{u} = \shortestResidualPath{f}{\nu} + 1$$
		and this again is a contradiction to our assumption. 
	\end{enumerate}
\end{proof}

\begin{Th}
	The number of iterations in the Edmonds-Karp algorithm is $O(VE)$. 
\end{Th}
\begin{proof}
	Let the minimum weight edge on an augmenting path be called the \textit{critical edge} of the path. We will show that each edge of $G$ can be critical atmost $O(V)$ times, and this will complete the proof. 
	
	Suppose the current flow is $f$, and we are augmenting a path that has critical edge $(u, v)$ (note that either $(u, v)\in E$ or $(v, u)\in E$, since residual graphs only have edges with positive flow). Suppose the new flow after augmenting the flow $f$ is $f'$. Since we are augmenting the edge $(u, v)$ of $G_f$, the new residual network $G_{f'}$ won't contain the edge $(u, v)$; to prove this, we consider the following two cases. 
	\begin{enumerate}
		\item In the first case, suppose $(u, v)\in E$. Here, since $(u, v)\in G_f$ and is the critical edge of the augmenting path, after augmentation, the flow on $(u, v)$ will be \textit{saturated}, i.e it will reach its full capacity. Hence, in the new residual $G_{f'}$, the edge $(u, v)$ won't be present. 
		\item In the second case, suppose $(u, v)\notin E$. The only way $(u, v)$ can be in $G_f$ is when $(v, u)\in E$ and $\netflow{v}{u} > 0$. After augmenting this path, the new flow on the edge $(v, u)$ will become 
		$$f'(v, u) = f(v, u) - f_p(u, v) = 0$$
		where above $p$ is the augmenting path. So, in the new residual graph $G_{f'}$, edge $(u, v)$ won't be present.
	\end{enumerate}
	Since we are augmenting the edge $(u, v)$ on the flow $f$, we have 
	$$\shortestResidualPath{v}{f} = \shortestResidualPath{u}{f} + 1$$
	Now, suppose the edge $(u, v)$ is critical \textit{again} for some flow $f''$. Since $f'$, the flow immediately after $f$, does not contain the edge $(u, v)$, the only way for $f''$ to contain the edge $(u, v)$ again is when the edge $(v, u)$ was augmented at some point before the flow $f''$ was obtained. Suppose $f'''$ is the flow after $f$ and before $f''$ which augmented the edge $(v, u)$. So, 
	$$\shortestResidualPath{u}{f'''} = \shortestResidualPath{v}{f'''} + 1 \ge \shortestResidualPath{v}{f} + 1 = \shortestResidualPath{u}{f} + 2$$
	where above we have used the monotonicity of $\delta_f$. This proves that between two instances of $(u, v)$ to be a critical edge, $\shortestResidualPath{}{u}$ increases by atleast $2$. Since this value is bounded above by $|V|$, we see that each such edge can be a critical edge atmost $|V|/2 = O(V)$ times. Hence, over all edges, the number of iterations is atmost $O(VE)$, and this proves the claim. 
\end{proof}

\subsection{Bipartite Matching} We can use bipartite matchings to solve the \textit{maximum bipartite matching problem}. Suppose we have a bipartite graph $G = (V, E)$. Our goal is to find a \textit{maximum matching} on this graph. We will see how to frame this problem as a max flow problem on the graph. The algorithm is quite simple, and we now describe it.

Suppose the graph is partitioned into sets $L, R$ ($L$ is the set of the vertices in the left partition, and similarly $R$ is the set of the vertices in the right partition). We create a corresponding flow network $G'$ as follows: introduce new vertices $s$ and $t$. For each $u\in L$, create a directed edge $(s, u)$. For each $v\in R$, create a directed edge $(v, t)$. Assign a capacity of $1$ to all the edges. 

Then, any \textit{integer valued} flow in this graph corresponds to a matching. This is not  hard to show (try to do it as an exercise). Moreover, if we use Ford-Fulkerson on this graph, all flows obtained in the intermediate steps will be \textit{integer values}; this is the so called \textit{integrality theorem}, and this is not hard to show either (intuitively, at each step, all updates are integer valued, assuming the flow and the capacities are integers). Moreover, it is true that the max flow in this graph is equal to the cardinality of the maximum matching in the graph (again, try to prove all these facts).

It follows that Ford-Fulkerson will give us the maximum matching. Note that a maximum matching can have size atmost $O(V)$, and hence Ford-Fulkerson runs in time $O(VE)$ on such a graph.  

\subsection{Dinic's Algorithm and Blocking Flows} A good resource for learning this is the following: \url{http://timroughgarden.org/w16/l/l2.pdf} 

Now, we will explore a faster max flow algorithm, namely \textit{Dinic's Algorithm}. This algorithm runs in time $O(V^2E)$ instead of the usual $O(VE^2)$, and clearly this is a faster algorithm.

The algorithm is pretty similar to Edmonds-Karp. 
\begin{enumerate}
	\item We start with a zero flow, i.e all edges have flow equal to $0$.
	\item While there is an $s-t$ path in the residual graph $G_f$, we construct the \textit{layered graph} $L_f$ (which will be defined in a moment) from the residual graph $G_f$ via BFS.
	\item We'll then compute the \textit{blocking flow} $g$ in the layered graph $L_f$ (again, we'll define this in a moment), and we augment the flow $f$ by $g$ (we'll see how to compute $g$).
\end{enumerate}

\begin{Remark}
	Note that we are computing the blocking flow in the graph $L_f$ (the layered graph) and not $G_f$; this is a really important but subtle thing to note. 
\end{Remark}

\begin{Def}
	Let $G$ be a flow network with a given flow $f$. Let $G_f$ be the residual graph. Assume that there is an $s-t$ path in $G_f$. The \textit{layered graph} $L_f$ is defined as follows: in the residual graph $G_f$, run a BFS from the source vertex $s$ until the vertex $t$ is reached. Let $\delta(s, t)$ denote the size of the \textit{shortest path} from $s$ to $t$. Then, for each $0\le i\le \delta(s, t)$, define the $i$th \textit{layer} to be the collection of all those vertices which are at a shortest distance of $i$ from $s$. Then, define $L_f$ to be the subgraph of $G_f$ consisting only of those edges of $G_f$ which go from layer $j$ to layer $j + 1$ for some $j$ (so we are not considering edges within the same layer and edges that go back). 
\end{Def}
So, the layered graph $L_f$ can really be thought of as \textit{layers} of vertices; the first layer consist of those vertices in $L_f$ that are at a distance of $1$ from $s$, and similarly for other layers. Clearly, all \textit{shortest paths} from $s$ to $t$ in graph $G_f$ are paths from $s$ to $t$ in $L_f$. Next, let us define \textit{blocking flows}. 

\begin{Def}
	A \textit{blocking flow} $g$ on a network $G$ is a flow such that, for every $s-t$ path $p$ of $G$, some edge of $p$ is saturated by $g$, i.e $f(u, v) = c(u, v)$ for some edge $(u, v)$ in $G$. So intuitively, a \textit{blocking flow} cancels out atleast one edge in \textit{every} $s-t$ path. 
\end{Def}
Just like in the proof of Edmonds-Karp, we will prove an assertion about shortest paths increasing if we augment our flow by a \textit{blocking flow}. First, we will prove a simple lemma. 

\begin{Lemma}
	\label{validDistanceLabel}
	Let $d(i)$ denote the shortest distance of vertex $i$ to the source $s$ in the graph $G_f$. Then, $d$ is a valid distance labelling function on $G_h$; i.e, for all edges $(i, j)\in G_h$, we have
	$$d(j)\le d(i) + 1$$
\end{Lemma}
\begin{proof}
	Suppose $(i, j)$ is an edge of $G_h$. Clearly, either $(i, j)\in G_f$ or $(i, j)\notin G_f$. We will handle both of these cases. 
	\begin{enumerate}
		\item First, suppose $(i, j)\in G_f$. In this case, there is nothing to prove, because $d_j \le d_i + 1$ in this case by the definition of shortest paths. 
		\item In the second case, we have that $(i, j)\notin G_f$. Then, the only way $(i, j)$ is introduced in $G_h$ is if we sent some flow on the edge $(j, i)$ in the previous iteration, i.e $(j, i)\in G_f$. Since $g$ is a blocking flow on $L_f$, this clearly means that $(j, i)\in L_f$, which in turn means that 
		$$d(j) = d(i) - 1 \le d(i) + 1$$
	\end{enumerate}
\end{proof}


\begin{Prop}
	Let $G$ be a flow network, and let $f$ be a flow on $G$. Consider the graph $G_f$, the residual graph. Let $d(f)$ denote the shortest distance between $s$ and $t$ in $G_f$. Let $g$ be a blocking flow in $L_f$ (the layered graph). If $h = f + g$, i.e if flow $h$ is obtained by augmenting $f$ by $g$, then $d(h) > d(f)$. So, by augmenting a flow with a blocking flow, the shortest $s-t$ distance strictly increases.
\end{Prop}
\begin{proof}
 	Let $d$ be the distance from the source $s$ in $G_f$, and let $d'$ be the distance from the source $s$ in $G_h$. We will show that  
	$$d(t) < d'(t)$$
	i.e after every iteration, the shortest $s-t$ distance strictly increases. 
	
	First, let $P$ be \textit{any} $s-t$ path in the residual graph $G_h$. We claim that there is some edge $(u, v)\in P$ in the residual graph $G_h$ such that either $(u, v)\notin G_f$ or $d(v)\ne d(u) + 1$. We now prove this. Suppose all the edges in the path $P$ are \textit{old} edges, i.e if $(u, v)\in P$, then $(u, v)\in G_f$. For the sake of contradiction, suppose all the edges $(u, v)\in P$ satisfy $d(v) = d(u) + 1$. Then, it is clear that $P$ is actually a shortest $s-t$ path in $G_f$; moreover, all the edges of $P$ are actually edges in the layered graph $L_f$. Now, since $g$ is a \textit{blocking flow} on the layered graph $L_f$, there is some edge $(u, v)\in P$ that is completely saturated. But, this would imply that, after augmenting $f$ by $g$ (to obtain $h$), the edge $(u, v)$ won't be present in the new residual graph, i.e $(u, v)\notin G_h$; this is a contradiction.
	
	We now prove the main statement of the proposition, i.e $d(t) < d'(t)$. Again, let $P$ be any $s-t$ path in $G_h$. So, as we've shown above, there is some edge $(u, v)\in P$ such that either $(u, v)\notin G_f$ or $d(v)\ne d(u) + 1$. In the first case, i.e $(u, v)\notin G_f$, the only way the edge $(u, v)$ is present in $G_h$ is if we have sent some flow on the edge $(v, u)$; clearly, this implies that $(v, u)\in L_f$, which means that $d(v) < d(u)$. In the other case, namely that $d(v)\ne d(u) + 1$, we clearly have that $d(v)\le d(u)$ since by \textbf{Lemma} \ref{validDistanceLabel}, we have that $d(v)\le d(u) + 1$. In particular, we have shown that for any such path $P$, there is some edge $(u, v)\in P$ such that $d(v)\le d(u)$. This clearly implies that $|P| > d(t)$, and hence this means that $d'(t) > d(t)$, completing the proof. 
\end{proof}
\begin{Cor}
	Dinic's algorithm takes atmost $O(V)$ iterations to run, and the time complexity of the algorithm is $O(V\cdot BF)$, where $BF$ is the time required to compute the blocking flow. 
\end{Cor}
\begin{proof}
	This is clear; because the shortest $s-t$ distance is strictly increasing at after every iteration, there can be atmost $O(V)$ iterations since the maximum possible shortest distance is $O(V)$. The second statement clearly follows from the first. 
\end{proof}
\begin{Remark}
	It follows that solving the blocking flow problem efficiently will give us a good algorithm to compute max flows. In the upcoming sections, we will see how to compute the blocking flow in $O(VE)$ time, giving us $O(V^2E)$ running time for Dinic's Algorithm. 
\end{Remark}

\subsection{Computing Blocking Flows Efficiently} Now we will see how to compute blocking flows efficiently in the layered graph $L_f$. The idea is simple: we will start a DFS from the source $s$. If at any point of time we reach $t$, we know that we have found a shortest $s-t$ path (since we are working in the graph $L_f$); we will then take the smallest capacity edge on this path, and we will augment the flow along this path. We will then repeat the same procedure, and we will make sure \textit{not to repeat edges} (which will be clear in the pseudocode). This is important because if we repeat edges, the time complexity wouldn't be $O(VE)$. To achieve this, for each vertex we will remember a \textit{pointer} which will store information about which edge of the vertex to process next. Please have a look at the pseudocode.

Convince yourself that computing a blocking flow using the given procedure takes time $O(VE)$; as a hint, note that each augmentation causes an edge in $L_f$ to be completely saturated. 

\begin{algorithm}
	\caption{Computing Blocking Flows}
	\begin{algorithmic}[1]
		\State \textbf{Input}: Network $G$, layered graph $L_f$ (computed using BFS). 
		\State For each $v$, $\text{degree}[v]$ denotes the number of edges from $v$ to the next level in $L_f$.
		\State $N[v][i]$ denotes the $i$th neighbor of vertex $v$, where $0\le i < \text{degree}[v]$. 
		\State $C[v]$ denotes the index of \textit{current} neighbor of $v$. Current neighbor is $N[v][C[v]]$. 
		\State $\text{Res}[v][i]$ denotes  capacity of the $i$th edge of $v$ in $L_f$, computed beforehand. Note that these really are the residual capacities of the original edges in $G$. The result will be returned by decreasing these capacities.  
		\State 
		\State 
		\State $\text{flow} \gets 0$. 
		\For {each vertex $v$}
			\State $C[v] = 0$
		\EndFor
		
		\While{\textbf{true}}
			\State $f\gets \text{DFS}(s)$
			\If {f = 0}
				\State \textbf{return} \text{flow} \Comment{Return the value of the blocking flow}
			\Else
				\State $\text{flow} \gets \text{flow} + f$
				\State Augment the current $s-t$ path by $f$, i.e reduce all capacities on this \State path by $f$. The current path is just $s\to N[s][C[s]]\to \cdots \to t$. 
			\EndIf
		\EndWhile
		
		\Function{dfs}{v}
			\State\Comment{Search a path from $v$ to $t$ of positive capacity.}
			\State\Comment{If there is no such path, return 0.}
			\State\Comment{As a side effect, this may increase $C[v]$.}
			
			\If{$v = t$}
				\State \textbf{return} $\infty$
			\EndIf
			
			\While{\textbf{true}}
				\If{$C[v] = \text{degree}[v]$}
					\State \textbf{return} $0$ \Comment{$v$ is \textit{dead}}
				\EndIf
				
				\If{$\text{Res}[v][C[v]] = 0$}
					\State $C[v] \gets C[v] + 1$ \Comment{Move to the next neighbor}
				\Else
					\State$f\gets \text{DFS}(N[v][C[v]])$
					\If{$f > 0$}	
						\State\textbf{return} $\min (f, \text{Res}[v][C[v]])$
					\Else
						\State $C[v]\gets C[v] + 1$
					\EndIf
				\EndIf
			\EndWhile
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\section{Push-Relabel Paradigm} 
\subsection{Preflow} We will begin by defining a \textit{preflow}.

\begin{Def}
	A \textit{preflow} on a flow network $G$ is a function $f:V\times V\to \textbf{R}$ that satisfies the capacity constraint, and also satisfies a relaxed version of the conservation constraint. Formally, for all $(u, v)\in E$, we have $$0\le f(u, v)\le c(u, v)$$
	and also for every $u\in V-\{s\}$, it is true that 
	$$\sum_{v\in V}f(v, u)\ge 0$$
	Ofcourse, we are still assuming that $f$ is anti-symmetric. The above equation just means that the flow into a vertex could be more than the flow out of a vertex (except the source $s$). This quantity is also called the \textit{excess flow} into vertex $u$. For a \textit{preflow} $f$, the \textit{residual network} $G_f$ is defined as before. 
\end{Def}

\textcolor{red}{Watch lecture from 10:30 mark.}
\end{document}  